from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import os
from typing import List, Dict, AsyncGenerator, Any
from dotenv import load_dotenv
from app.database.chat_history_service import save_chat_history, get_recent_chat_history, format_chat_history
from pydantic import BaseModel, Field
from langchain_core.messages import AIMessageChunk
from langchain.callbacks.base import BaseCallbackHandler
from .tools import (
    CheckPropertiesDistrictTool, 
    CheckPropertiesStatusTool,
    CheckPropertiesPriceRangeTool,
    ShowPropertiesTool, 
    DucbaCheckingLocationTool,
    TanSonNhatCheckingLocationTool,
    UniversityCheckingLocationTool
)
import time

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

# Create tools
# product_search_tool = ProductSearchTool()
check_properties_district_tool = CheckPropertiesDistrictTool()
check_properties_status_tool = CheckPropertiesStatusTool()
check_properties_price_range_tool = CheckPropertiesPriceRangeTool()
show_properties_tool = ShowPropertiesTool()
ducba_checking_location_tool = DucbaCheckingLocationTool()
tansonhat_checking_location_tool = TanSonNhatCheckingLocationTool()
university_checking_location_tool = UniversityCheckingLocationTool()
# create_order_tool = CreateOrderTool()
# update_order_status_tool = UpdateOrderStatusTool()

class CustomHandler(BaseCallbackHandler):
    """
    L·ªõp x·ª≠ l√Ω callback t√πy ch·ªânh ƒë·ªÉ theo d√µi v√† x·ª≠ l√Ω c√°c s·ª± ki·ªán trong qu√° tr√¨nh chat
    """
    def __init__(self, stream_delay: float = 0.1):
        """
        Args:
            stream_delay (float): Th·ªùi gian delay gi·ªØa c√°c token (gi√¢y)
        """
        super().__init__()
        self.stream_delay = stream_delay
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """ƒê∆∞·ª£c g·ªçi khi c√≥ token m·ªõi t·ª´ LLM."""
        time.sleep(self.stream_delay)  # Th√™m delay gi·ªØa c√°c token

def get_llm_and_agent() -> AgentExecutor:
    system_message = """You are a real estate assistant for Ho Chi Minh City. Match user's language (VN/EN).

DISPLAY FORMAT:
```
üè† [Name]
üìç [Address], [District]
üí∞ [Price]M VND/month | üèó [Area]m¬≤
üõè [Bedrooms] beds | üöø [Bathrooms] baths
üì∏ Images:
[Direct Image Display]
üìû Contact: [Name] - [Phone]
```

SEARCH PATTERNS:
1. Location: "in [District]", "near [Landmark]"
2. Price: "under [X]M", "[X]-[Y]M"
3. Combined: "in [District] under [X]M near [Landmark]"

KEY LOCATIONS:
‚Ä¢ Airport: 10.818663¬∞N, 106.654835¬∞E
‚Ä¢ Cathedral: 10.779814¬∞N, 106.699150¬∞E
‚Ä¢ Universities: HCMUS(Q5), HCMUT(Q10), HUTECH(BT)

DISTRICT ALIASES:
{
  "q1,quan 1,district 1": "Qu·∫≠n 1",
  "tan binh,qtb": "T√¢n B√¨nh",
  "binh thanh,qbt": "B√¨nh Th·∫°nh"
}

RESPONSE RULES:
1. Always show images directly
2. Sort by relevance (distance/price)
3. Include total count
4. Show contact info
5. If no results, suggest alternatives"""

    chat = ChatOpenAI(
        temperature=0.7,  
        streaming=True, 
        model="gpt-4o-mini",
        api_key=OPENAI_API_KEY,
        request_timeout=40,  
        callbacks=[CustomHandler(stream_delay=0.07)] 
    )
    
    tools = [
        show_properties_tool,
        check_properties_district_tool,
        check_properties_status_tool,
        check_properties_price_range_tool,
        ducba_checking_location_tool,
        tansonhat_checking_location_tool,
        university_checking_location_tool
    ]

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    agent = create_openai_functions_agent(
        llm=chat,
        tools=tools,
        prompt=prompt
    )

    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=False,
        return_intermediate_steps=True
    )

    return agent_executor

def get_answer(question: str, thread_id: str) -> Dict:
    """
    H√†m l·∫•y c√¢u tr·∫£ l·ªùi cho m·ªôt c√¢u h·ªèi
    
    Args:
        question (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        thread_id (str): ID c·ªßa cu·ªôc tr√≤ chuy·ªán
        
    Returns:
        str: C√¢u tr·∫£ l·ªùi t·ª´ AI
    """
    agent = get_llm_and_agent()
    
    # Get recent chat history
    history = get_recent_chat_history(thread_id)
    chat_history = format_chat_history(history)
    
    result = agent.invoke({
        "input": question,
        "chat_history": chat_history
    })
    
    # Save chat history to database
    if isinstance(result, dict) and "output" in result:
        save_chat_history(thread_id, question, result["output"])
    
    return result

async def get_answer_stream(question: str, thread_id: str) -> AsyncGenerator[Dict, None]:
    """
    H√†m l·∫•y c√¢u tr·∫£ l·ªùi d·∫°ng stream cho m·ªôt c√¢u h·ªèi
    
    Quy tr√¨nh x·ª≠ l√Ω:
    1. Kh·ªüi t·∫°o agent v·ªõi c√°c tools c·∫ßn thi·∫øt
    2. L·∫•y l·ªãch s·ª≠ chat g·∫ßn ƒë√¢y
    3. G·ªçi agent ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi
    4. Stream t·ª´ng ph·∫ßn c·ªßa c√¢u tr·∫£ l·ªùi v·ªÅ client
    5. L∆∞u c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh v√†o database
    
    Args:
        question (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        thread_id (str): ID phi√™n chat
        
    Returns:
        AsyncGenerator[str, None]: Generator tr·∫£ v·ªÅ t·ª´ng ph·∫ßn c·ªßa c√¢u tr·∫£ l·ªùi
    """
    # Kh·ªüi t·∫°o agent v·ªõi c√°c tools c·∫ßn thi·∫øt
    agent = get_llm_and_agent()
    
    # L·∫•y l·ªãch s·ª≠ chat g·∫ßn ƒë√¢y
    history = get_recent_chat_history(thread_id)
    chat_history = format_chat_history(history)
    
    # Bi·∫øn l∆∞u c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
    final_answer = ""
    
    # Stream t·ª´ng ph·∫ßn c·ªßa c√¢u tr·∫£ l·ªùi
    async for event in agent.astream_events(
        {
            "input": question,
            "chat_history": chat_history,
        },
        version="v2"
    ):       
        # L·∫•y lo·∫°i s·ª± ki·ªán
        kind = event["event"]
        # N·∫øu l√† s·ª± ki·ªán stream t·ª´ model
        if kind == "on_chat_model_stream":
            # L·∫•y n·ªôi dung token
            content = event['data']['chunk'].content
            if content:  # Ch·ªâ yield n·∫øu c√≥ n·ªôi dung
                # C·ªông d·ªìn v√†o c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
                final_answer += content
                # Tr·∫£ v·ªÅ token cho client
                yield content
    
    # L∆∞u c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh v√†o database
    if final_answer:
        save_chat_history(thread_id, question, final_answer)

if __name__ == "__main__":
    import asyncio
    
    async def test():
        # answer = get_answer_stream("hi", "test-session")
        # print(answer)
        async for event in get_answer_stream("hi", "test-session"):
            print('event:', event)
        print('done')

    
    asyncio.run(test())


